{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707db457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd02d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BBC News.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aacdea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = text.lower().replace('\\n',' ').replace('\\r','').strip()\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    \n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    \n",
    "    text = \" \".join(filtered_sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356693dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text_parsed'] = data['Text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429084e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "data['Category_target']= label_encoder.fit_transform(data['Category']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930e2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('BBC_News_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59fd046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['Text_parsed'], \n",
    "                                                    data['Category_target'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e384807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eed5fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "# print(features_train)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "# print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ce1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model  = RandomForestClassifier(random_state=1)\n",
    "model.fit(features_train, labels_train)\n",
    "model_predictions = model.predict(features_test)\n",
    "# print('Accuracy: ', accuracy_score(labels_test, model_predictions))\n",
    "# print(classification_report(labels_test, model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d39effb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e045847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "n_estimators = [100, 300, 500, 800, 1200]\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "min_samples_leaf = [1, 2, 5, 10] \n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "              min_samples_split = min_samples_split, \n",
    "             min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "gridF = GridSearchCV(model, hyperF, cv = 3, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "bestF = gridF.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f997e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 30,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da90051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model1  = RandomForestClassifier(random_state=1,max_depth= 30, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 100)\n",
    "model1.fit(features_train, labels_train)\n",
    "model_predictions = model1.predict(features_test)\n",
    "# print('Accuracy: ', accuracy_score(labels_test, model_predictions))\n",
    "# print(classification_report(labels_test, model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6832e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65bcaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('LIARtest.csv')\n",
    "train = pd.read_csv('LIARtrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44dcfb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5012 entries, 0 to 5011\n",
      "Columns: 131 entries, Column 2 to Unnamed: 130\n",
      "dtypes: float64(46), int64(1), object(84)\n",
      "memory usage: 5.0+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1267 entries, 0 to 1266\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Column 2    1267 non-null   int64 \n",
      " 1   Label       1267 non-null   object\n",
      " 2   Statement   1267 non-null   object\n",
      " 3   Statement1  1258 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 39.7+ KB\n",
      "Cleaning over..\n"
     ]
    }
   ],
   "source": [
    "def Data_Clean():\n",
    "  print(\"Processing...\")\n",
    "# Train\n",
    "  train.isnull().sum()\n",
    "  train.info()\n",
    "# Test\n",
    "  test.isnull().sum()\n",
    "  test.info()\n",
    "Data_Clean()\n",
    "print(\"Cleaning over..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b9fb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the original Data for future\n",
    "train_orig = train.copy()\n",
    "test_orig = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c136593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(str_tokens, str_stemmer):\n",
    "  stemmed_str = []\n",
    "  for token_i in str_token:\n",
    "    stemmed.append(str_stemmer.stem(token_i))\n",
    "  return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76cc8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data process\n",
    "def data_process(data, exclude_stopword = True, stem = True):\n",
    "  str_tokens = [w.lower() for w in data]\n",
    "  stemmed_tokens = str_tokens\n",
    "  stemmed_tokens = stemming(str_tokens, eng_stemmer)\n",
    "  stemmed_tokens = [w for w in stemmed_tokens if w not in stopwords ]  \n",
    "  return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56199af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating ngrams\n",
    "#unigram \n",
    "def create_unigram(words):\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "#bigram\n",
    "def create_bigrams(words):\n",
    "    assert type(words) == list\n",
    "    gap = 0\n",
    "    joinned_str = \" \"\n",
    "    Len = len(words)\n",
    "    if Len > 1:\n",
    "        lst = []\n",
    "        for i in range(Len-1):\n",
    "            for k in range(1,gap+2):\n",
    "                if i+k < Len:\n",
    "                    lst.append(joinned_str.join([words[i],words[i+k]]))\n",
    "    else:\n",
    "        #set it as unigram\n",
    "        lst = create_unigram(words)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0da6894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram creation\n",
    "def create_trigrams(words):\n",
    "    assert type(words) == list\n",
    "    gap == 0\n",
    "    joinned_str = \" \"\n",
    "    Len = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(1,gap+2):\n",
    "            for k1 in range(1, gap+2):\n",
    "                for k2 in range(1,gap+2):\n",
    "                    if i+k1 < Len and i+k1+k2 < Len:\n",
    "                        lst.append(joinned_str.join([words[i], words[i+k1],words[i+k1+k2]]))\n",
    "        else:\n",
    "            #set is as bigram\n",
    "            lst = create_bigram(words)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e4a7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_str = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32c9f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_tokenizer(statement):\n",
    "    return [porter.stem(word) for word in statement.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be13f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0aa06b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "  (0, 6937)\t1\n",
      "  (0, 7856)\t1\n",
      "  (0, 725)\t1\n",
      "  (0, 4756)\t1\n",
      "  (0, 5981)\t1\n",
      "  (0, 3660)\t1\n",
      "  (0, 7660)\t1\n",
      "  (0, 7885)\t1\n",
      "  (0, 8078)\t1\n",
      "  (0, 425)\t1\n",
      "  (0, 5529)\t1\n",
      "  (0, 2350)\t1\n",
      "  (1, 7856)\t1\n",
      "  (1, 8544)\t2\n",
      "  (1, 2465)\t1\n",
      "  (1, 2274)\t1\n",
      "  (1, 5492)\t1\n",
      "  (1, 1762)\t1\n",
      "  (1, 7455)\t1\n",
      "  (1, 4335)\t1\n",
      "  (1, 7456)\t2\n",
      "  (1, 5314)\t1\n",
      "  (1, 3479)\t1\n",
      "  (1, 7962)\t1\n",
      "  (1, 5493)\t1\n",
      "  :\t:\n",
      "  (5011, 7856)\t1\n",
      "  (5011, 5492)\t2\n",
      "  (5011, 7939)\t2\n",
      "  (5011, 5611)\t1\n",
      "  (5011, 5608)\t1\n",
      "  (5011, 3759)\t1\n",
      "  (5011, 4528)\t1\n",
      "  (5011, 8690)\t2\n",
      "  (5011, 810)\t1\n",
      "  (5011, 7869)\t1\n",
      "  (5011, 2376)\t1\n",
      "  (5011, 8696)\t1\n",
      "  (5011, 7862)\t1\n",
      "  (5011, 8338)\t2\n",
      "  (5011, 4730)\t1\n",
      "  (5011, 558)\t1\n",
      "  (5011, 1816)\t1\n",
      "  (5011, 6437)\t1\n",
      "  (5011, 7620)\t1\n",
      "  (5011, 8309)\t1\n",
      "  (5011, 2843)\t1\n",
      "  (5011, 7571)\t1\n",
      "  (5011, 1833)\t1\n",
      "  (5011, 7802)\t1\n",
      "  (5011, 4915)\t1\n"
     ]
    }
   ],
   "source": [
    "# creating word matrix  with bag of word technique\n",
    "countV = CountVectorizer()\n",
    "train_count = countV.fit_transform(train['Statement'].values)\n",
    "print(countV)\n",
    "print(train_count)\n",
    "\n",
    "def countVectorizer_matrix():\n",
    "        #vocab size\n",
    "    train_count.shape\n",
    "    #check vocabulary using below command\n",
    "    print(countV.vocabulary_)\n",
    "    #get feature names\n",
    "    print(countV.get_feature_names()[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec0ebb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-df features \n",
    "tfidfV = TfidfTransformer()\n",
    "train_tfidf = tfidfV.fit_transform(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3bb3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix():\n",
    "    train_tfidf.shape\n",
    "    #get train data feature names \n",
    "    print(train_tfidf.A[:10])\n",
    "#bag of words - with n-grams\n",
    "#tfidf_ngram  = TfidfTransformer(use_idf=True,smooth_idf=True)\n",
    "tfidf_ngram = TfidfVectorizer(stop_words='english',ngram_range=(1,4),use_idf = True,smooth_idf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98c9bd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\patra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Tags\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = train['Statement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cba8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_Scale(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf048a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5ffcdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import learning_curve,  GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54ae29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string to test\n",
    "statement_doc = ['We are already almost halfway to our 2010 goal of creating 700,000 new jobs in seven years.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57a7ed8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27229676400947117"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "random_forest = Pipeline([\n",
    "        ('rfCV',countV),\n",
    "        ('rf_clf',RandomForestClassifier(n_estimators=350,n_jobs=3))\n",
    "        ])\n",
    "print(\"Accuracy: \")    \n",
    "random_forest.fit(train['Statement'],train['Label'])\n",
    "predicted_rf = random_forest.predict(test['Statement'])\n",
    "np.mean(predicted_rf == test['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce0535aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38a02faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the news text you want to test or verify: Ashwatthama was a formidable warrior who fought on the side of the Kauravas during the great battle at Kurukshetra.\n",
      "The category of the news is: Sport\n",
      "Folowing Statement predicted as:   FALSE\n",
      "The truth probability score will be:   0.335\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "import pickle\n",
    "Var = input(\"Enter the news text you want to test or verify: \")\n",
    "def classify_fake_news(Var):    \n",
    "    load_model = pickle.load(open('Model.sav', 'rb'))\n",
    "    features_test = tfidf.transform([Var]).toarray()\n",
    "    return print(\"The category of the news is:\",str(list(model1.predict(features_test))[0]).replace('0', 'Business').replace('1', 'Entertainment').replace('2', 'Politics').replace('3', 'Sport').replace('4', 'Tech'))\n",
    "#if __name__ == '__main__':\n",
    "classify_fake_news(Var)\n",
    "#import pickle\n",
    "#var = input(\"Enter the news text you want to test or verify: \")\n",
    "#print(\"Your Entry: \" + str(var))\n",
    "# Here we go with the prediction:\n",
    "def detecting_fake_news(Var):    \n",
    "    load_model = pickle.load(open('Model.sav', 'rb'))\n",
    "    prediction = load_model.predict([Var])\n",
    "    prob = load_model.predict_proba([Var])\n",
    "    return (print(\"Folowing Statement predicted as:  \",prediction[0]),\n",
    "        print(\"The truth probability score will be:  \",prob[0][1]))\n",
    "if __name__ == '__main__':\n",
    "    detecting_fake_news(Var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4b0974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the news text you want to test or verify: Ashwatthama was a formidable warrior who fought on the side of the Kauravas during the great battle at Kurukshetra.\n",
      "The category of the news is: Sport\n",
      "Folowing Statement predicted as:   FALSE\n",
      "The truth probability score will be:   0.335\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "import pickle\n",
    "Var = input(\"Enter the news text you want to test or verify: \")\n",
    "def classify_fake_news(Var):    \n",
    "    load_model = pickle.load(open('Model.sav', 'rb'))\n",
    "    features_test = tfidf.transform([Var]).toarray()\n",
    "    return print(\"The category of the news is:\",str(list(model1.predict(features_test))[0]).replace('0', 'Business').replace('1', 'Entertainment').replace('2', 'Politics').replace('3', 'Sport').replace('4', 'Tech'))\n",
    "#if __name__ == '__main__':\n",
    "classify_fake_news(Var)\n",
    "#import pickle\n",
    "#var = input(\"Enter the news text you want to test or verify: \")\n",
    "#print(\"Your Entry: \" + str(var))\n",
    "# Here we go with the prediction:\n",
    "def detecting_fake_news(Var):    \n",
    "    load_model = pickle.load(open('Model.sav', 'rb'))\n",
    "    prediction = load_model.predict([Var])\n",
    "    prob = load_model.predict_proba([Var])\n",
    "    return (print(\"Folowing Statement predicted as:  \",prediction[0]),\n",
    "        print(\"The truth probability score will be:  \",prob[0][1]))\n",
    "if __name__ == '__main__':\n",
    "    detecting_fake_news(Var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3181c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
